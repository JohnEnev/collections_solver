{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluation\n",
    "\n",
    "Evaluate models on the test set using the game loop.\n",
    "\n",
    "Supports:\n",
    "- Local models (base Qwen, fine-tuned Alpha/Beta)\n",
    "- API models (Claude, GPT-4o, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "# Add parent dir for imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.evaluation import run_evaluation, load_puzzles, analyze_one_away_recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test puzzles\n",
    "test_puzzles = load_puzzles(\"data/test.jsonl\")\n",
    "print(f\"Loaded {len(test_puzzles)} test puzzles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Evaluate Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base Qwen (no LoRA)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-14B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Base Qwen loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run base model evaluation\n",
    "results_base = run_evaluation(\n",
    "    model, tokenizer, test_puzzles,\n",
    "    model_name=\"base\",\n",
    "    output_dir=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alpha (fine-tuned on golden traces)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-14B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"qwen-connections-alpha-r32-ep3\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Verify LoRA loaded\n",
    "print(f\"Adapters: {model.active_adapters}\")\n",
    "print(\"Alpha loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Alpha evaluation\n",
    "results_alpha = run_evaluation(\n",
    "    model, tokenizer, test_puzzles,\n",
    "    model_name=\"alpha\",\n",
    "    output_dir=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Beta (fine-tuned on golden + game traces)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-14B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"qwen-connections-beta-r32-ep2.5\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Beta loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Beta evaluation\n",
    "results_beta = run_evaluation(\n",
    "    model, tokenizer, test_puzzles,\n",
    "    model_name=\"beta\",\n",
    "    output_dir=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Evaluate API Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For API models, we need a modified game loop\n",
    "# See src/api_evaluation.py for full implementation\n",
    "\n",
    "from anthropic import Anthropic\n",
    "# from openai import OpenAI\n",
    "\n",
    "ANTHROPIC_API_KEY = \"sk-ant-...\"  # Your key\n",
    "client = Anthropic(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API game loop (simplified)\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(text):\n",
    "    matches = list(re.finditer(r'```json\\s*(\\{.*?\\})\\s*```', text, re.DOTALL))\n",
    "    if matches:\n",
    "        try:\n",
    "            return json.loads(matches[-1].group(1))\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        start = text.rfind('{')\n",
    "        if start != -1:\n",
    "            depth = 0\n",
    "            for i, c in enumerate(text[start:]):\n",
    "                if c == '{': depth += 1\n",
    "                elif c == '}': depth -= 1\n",
    "                if depth == 0:\n",
    "                    return json.loads(text[start:start + i + 1])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ... (full implementation in src/api_evaluation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ONE AWAY recovery rates\n",
    "print(\"ONE AWAY Recovery Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name in [\"base\", \"alpha\", \"beta\"]:\n",
    "    traces_file = f\"data/eval_{model_name}_traces.jsonl\"\n",
    "    try:\n",
    "        stats = analyze_one_away_recovery(traces_file)\n",
    "        print(f\"{model_name.upper()}:\")\n",
    "        print(f\"  ONE AWAY situations: {stats['total']}\")\n",
    "        print(f\"  Recovered: {stats['recovered']} ({stats['recovery_rate']:.1f}%)\")\n",
    "        print()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{model_name}: No traces file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a trace\n",
    "def show_trace(traces_file, puzzle_id):\n",
    "    \"\"\"Display a specific game trace.\"\"\"\n",
    "    with open(traces_file) as f:\n",
    "        for line in f:\n",
    "            t = json.loads(line)\n",
    "            if t[\"puzzle_id\"] == puzzle_id:\n",
    "                status = \"✅ SOLVED\" if t[\"solved\"] else \"❌ LOST\"\n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"PUZZLE {puzzle_id} — {status}\")\n",
    "                print(f\"Groups: {t['groups_found']}/4, Mistakes: {t['mistakes']}\")\n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"\\nWords: {t['words']}\")\n",
    "                print(f\"\\nSolution:\")\n",
    "                for name, words in t['solution'].items():\n",
    "                    print(f\"  {name}: {words}\")\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(\"GAME TRACE:\")\n",
    "                for turn in t[\"trace\"]:\n",
    "                    print(f\"\\n--- Turn {turn.get('turn', '?')} ---\")\n",
    "                    if \"guess\" in turn:\n",
    "                        print(f\"Guess: {turn['guess']}\")\n",
    "                    print(f\"Result: {turn.get('result', turn.get('action', '?'))}\")\n",
    "                return\n",
    "    print(f\"Puzzle {puzzle_id} not found\")\n",
    "\n",
    "# Example: show a specific puzzle\n",
    "# show_trace(\"data/eval_beta_traces.jsonl\", 812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "results_summary = {\n",
    "    \"Base Qwen 14B\": {\"solve_rate\": 9.3, \"avg_groups\": 0.75},\n",
    "    \"GPT-4o-mini\": {\"solve_rate\": 10.0, \"avg_groups\": 0.78},\n",
    "    \"Claude Haiku 3.5\": {\"solve_rate\": 13.3, \"avg_groups\": 1.07},\n",
    "    \"GPT-4o\": {\"solve_rate\": 22.7, \"avg_groups\": 1.49},\n",
    "    \"Alpha (fine-tuned)\": {\"solve_rate\": 27.3, \"avg_groups\": 1.75},\n",
    "    \"Beta (fine-tuned)\": {\"solve_rate\": 30.0, \"avg_groups\": 1.91},\n",
    "    \"Claude Sonnet 4.5\": {\"solve_rate\": 87.3, \"avg_groups\": 3.61},\n",
    "}\n",
    "\n",
    "print(\"Final Results (150 test puzzles)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Model':<25} {'Solve Rate':>12} {'Avg Groups':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for model, stats in results_summary.items():\n",
    "    print(f\"{model:<25} {stats['solve_rate']:>11.1f}% {stats['avg_groups']:>11.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
