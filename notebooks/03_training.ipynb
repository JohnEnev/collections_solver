{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Model Training\n",
    "\n",
    "Fine-tune Qwen 2.5 14B using QLoRA with Unsloth.\n",
    "\n",
    "Two models:\n",
    "- **Alpha**: Trained on golden traces only (358 examples, 3 epochs)\n",
    "- **Beta**: Trained on golden + game traces (508 examples, 2.5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install unsloth transformers datasets trl peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "BASE_MODEL = \"unsloth/Qwen2.5-14B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# LoRA config\n",
    "LORA_RANK = 32  # Higher rank helped for this task\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training config\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 4  # Effective batch size = 8\n",
    "\n",
    "# Choose which model to train\n",
    "TRAIN_ALPHA = False  # Set True to train Alpha, False for Beta\n",
    "\n",
    "if TRAIN_ALPHA:\n",
    "    DATA_PATH = \"data/sft_alpha.jsonl\"\n",
    "    EPOCHS = 3\n",
    "    SAVE_NAME = \"qwen-connections-alpha-r32-ep3\"\n",
    "else:\n",
    "    DATA_PATH = \"data/sft_beta.jsonl\"\n",
    "    EPOCHS = 2.5  # 2.5 epochs worked better than 3 for beta\n",
    "    SAVE_NAME = \"qwen-connections-beta-r32-ep2.5\"\n",
    "\n",
    "OUTPUT_DIR = f\"outputs_{SAVE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = []\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} training examples from {DATA_PATH}\")\n",
    "\n",
    "# Count sources\n",
    "sources = {}\n",
    "for ex in data:\n",
    "    src = ex.get(\"metadata\", {}).get(\"source\", \"unknown\")\n",
    "    sources[src] = sources.get(src, 0) + 1\n",
    "print(f\"Sources: {sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Format using chat template\n",
    "def format_chat(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_chat)\n",
    "\n",
    "# Check a sample\n",
    "print(f\"\\nSample (first 500 chars):\")\n",
    "print(dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Training {SAVE_NAME}...\")\n",
    "print(f\"  Data: {len(data)} examples\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA weights\n",
    "model.save_pretrained(SAVE_NAME)\n",
    "tokenizer.save_pretrained(SAVE_NAME)\n",
    "print(f\"Model saved to {SAVE_NAME}\")\n",
    "\n",
    "# Also backup to /data for persistence (if using RunPod)\n",
    "import shutil\n",
    "backup_path = f\"/data/{SAVE_NAME}\"\n",
    "shutil.copytree(SAVE_NAME, backup_path, dirs_exist_ok=True)\n",
    "print(f\"Backed up to {backup_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = \"\"\"REMAINING: CAT, DOG, FISH, BIRD, RED, BLUE, GREEN, YELLOW, ONE, TWO, THREE, FOUR, APPLE, BANANA, ORANGE, GRAPE\n",
    "\n",
    "Find 4 words that share a hidden theme.\n",
    "\n",
    "Think step by step:\n",
    "1. What patterns do you see?\n",
    "2. Which 4-word group are you MOST confident about?\n",
    "3. Verify: Are all 4 words in the REMAINING list?\n",
    "\n",
    "Output your most confident group:\n",
    "{\"group\": [\"W1\", \"W2\", \"W3\", \"W4\"]}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"NYT Connections. Find groups of 4. Output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.3, do_sample=True)\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Reference\n",
    "\n",
    "Expected loss curves:\n",
    "\n",
    "**Alpha (358 examples, 3 epochs):**\n",
    "- Start: ~1.2-1.3\n",
    "- End: ~0.35-0.40\n",
    "\n",
    "**Beta (508 examples, 2.5 epochs):**\n",
    "- Start: ~1.2-1.3  \n",
    "- End: ~0.35-0.40\n",
    "\n",
    "If loss drops below 0.2, you may be overfitting. If starting loss is ~0.4, you accidentally loaded an already fine-tuned model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
